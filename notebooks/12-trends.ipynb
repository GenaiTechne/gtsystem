{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98d3e38-543f-446c-a88a-57945bfc1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from gtsystem import openai, bedrock, ollama, anthropic, render, tasks, instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba481a77-1403-46b3-9bbf-ac58036f42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.load('../data/optimize-genai-2024-0418.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2645ec31-c96c-4704-aaa2-689aee1bcf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2307e th {\n",
       "  font-size: 10pt;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2307e td {\n",
       "  text-align: left;\n",
       "  word-wrap: break-word;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       "#T_2307e .index_name {\n",
       "  display: none;\n",
       "}\n",
       "#T_2307e .row_heading {\n",
       "  display: none;\n",
       "}\n",
       "#T_2307e  .blank {\n",
       "  display: none;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2307e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2307e_level0_col0\" class=\"col_heading level0 col0\" >Task</th>\n",
       "      <th id=\"T_2307e_level0_col1\" class=\"col_heading level0 col1\" >System</th>\n",
       "      <th id=\"T_2307e_level0_col2\" class=\"col_heading level0 col2\" >Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2307e_row0_col0\" class=\"data row0 col0\" >Reduce LLM inference cost</td>\n",
       "      <td id=\"T_2307e_row0_col1\" class=\"data row0 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row0_col2\" class=\"data row0 col2\" >How to reduce LLM runtime or inference cost?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2307e_row1_col0\" class=\"data row1 col0\" >Improve LLM accuracy</td>\n",
       "      <td id=\"T_2307e_row1_col1\" class=\"data row1 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row1_col2\" class=\"data row1 col2\" >How to improve LLM accuracy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2307e_row2_col0\" class=\"data row2 col0\" >Improve LLM performance</td>\n",
       "      <td id=\"T_2307e_row2_col1\" class=\"data row2 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row2_col2\" class=\"data row2 col2\" >How to improve LLM performance?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2307e_row3_col0\" class=\"data row3 col0\" >Effectively deploy LLMs</td>\n",
       "      <td id=\"T_2307e_row3_col1\" class=\"data row3 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row3_col2\" class=\"data row3 col2\" >What to most effectively deploy LLMs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_2307e_row4_col0\" class=\"data row4 col0\" >Scale LLMs efficiently</td>\n",
       "      <td id=\"T_2307e_row4_col1\" class=\"data row4 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row4_col2\" class=\"data row4 col2\" >How to scale LLMs efficiently?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_2307e_row5_col0\" class=\"data row5 col0\" >Training data selection</td>\n",
       "      <td id=\"T_2307e_row5_col1\" class=\"data row5 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row5_col2\" class=\"data row5 col2\" >What to perform training data selection and curation for LLMs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_2307e_row6_col0\" class=\"data row6 col0\" >Ethical use of GenAI</td>\n",
       "      <td id=\"T_2307e_row6_col1\" class=\"data row6 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row6_col2\" class=\"data row6 col2\" >How to ensure the ethical use of Generative AI?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_2307e_row7_col0\" class=\"data row7 col0\" >Bias and fairness in GenAI</td>\n",
       "      <td id=\"T_2307e_row7_col1\" class=\"data row7 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row7_col2\" class=\"data row7 col2\" >What to handle bias and fairness in Generative AI models?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_2307e_row8_col0\" class=\"data row8 col0\" >LLM Explainability</td>\n",
       "      <td id=\"T_2307e_row8_col1\" class=\"data row8 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row8_col2\" class=\"data row8 col2\" >How to perform LLM interpretability and explainability?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_2307e_row9_col0\" class=\"data row9 col0\" >LLM adversarial attacks</td>\n",
       "      <td id=\"T_2307e_row9_col1\" class=\"data row9 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row9_col2\" class=\"data row9 col2\" >How to make Generative AI applications more robust against adversarial attacks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_2307e_row10_col0\" class=\"data row10 col0\" >LLM regulatory compliance</td>\n",
       "      <td id=\"T_2307e_row10_col1\" class=\"data row10 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row10_col2\" class=\"data row10 col2\" >How do we balance innovation with regulatory compliance of Generative AI applications?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2307e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_2307e_row11_col0\" class=\"data row11 col0\" >LLM emerging trends</td>\n",
       "      <td id=\"T_2307e_row11_col1\" class=\"data row11 col1\" >Only respond with a markdown table with the top 5 latest emerging trends or technologies ranked by order of most popular first. Include columns for Popularity, Emerging Trend or Technology, Description, Recommended by Experts (with 3 top industry analysts or Big5 consultants or VCs recommending the trend or technology), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_2307e_row11_col2\" class=\"data row11 col2\" >How to enhance our Generative AI application?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10fcf2c50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render.df(tasks.list(0, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd99c42-56fd-45b6-b602-42fb9b0d4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.get('Reduce LLM inference cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212901e2-2df9-44ef-9e2d-2a7623725aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Popularity | Strategy                          | Description                                                                 | Examples                                      | Paper                                                                                     |\n",
       "|------------|-----------------------------------|-----------------------------------------------------------------------------|-----------------------------------------------|-------------------------------------------------------------------------------------------|\n",
       "| 1          | Quantization                      | Reduces the model size and speeds up inference by using lower precision.    | - Post-training quantization<br>- Dynamic quantization<br>- Quantization aware training | [Quantization in Deep Learning](https://arxiv.org/abs/2207.05795)                         |\n",
       "| 2          | Pruning                           | Removes less important parameters from a model to reduce its complexity.    | - Magnitude pruning<br>- Structured pruning<br>- Iterative pruning         | [To Prune or Not to Prune: Exploring the Efficacy of Pruning for Model Compression](https://arxiv.org/abs/1710.01878) |\n",
       "| 3          | Distillation                      | Transfers knowledge from a large model to a smaller, more efficient model.  | - Soft target distillation<br>- Hard target distillation<br>- Self-distillation | [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)          |\n",
       "| 4          | Efficient Architectures           | Designing or using models that are inherently more efficient.               | - MobileBERT<br>- TinyBERT<br>- DistilBERT                                | [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](https://arxiv.org/abs/1908.08962) |\n",
       "| 5          | Caching Intermediate Representations | Stores intermediate results to avoid redundant computation during inference. | - Layer caching<br>- Attention caching<br>- Embedding caching             | [Caching for Recurrent Neural Network Language Models](https://ieeexplore.ieee.org/document/7953252) |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.md(openai.text(*task, tokens=1024, stream=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434ef8d1-8668-4f15-83a9-40c79333eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.get('Improve LLM accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33dee4e8-014c-4c43-9df5-041c826cb91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Popularity | Strategy | Description | Examples | Paper |\n",
       "| --- | --- | --- | --- | --- |\n",
       "| 1 | Prompt Engineering | Crafting effective prompts to guide the LLM's responses | 1) Using few-shot examples <br> 2) Providing instructions <br> 3) Iterative refinement | [Anthropic AI: Constitutional AI: Harmonic Reinforcement Learning for Aligning AI Systems with Human Preferences](https://cdn.openai.com/constitutional-ai.pdf) |\n",
       "| 2 | Fine-tuning | Further training the LLM on a specific domain or task | 1) Fine-tuning on medical data <br> 2) Fine-tuning on legal documents <br> 3) Fine-tuning on code repositories | [OpenAI: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) |\n",
       "| 3 | Ensembling | Combining outputs from multiple LLMs | 1) Majority voting <br> 2) Weighted averaging <br> 3) Stacking | [Google AI: Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://arxiv.org/abs/1910.10683) |\n",
       "| 4 | Reinforcement Learning | Training LLMs using reward signals from human feedback | 1) Reward modeling <br> 2) Preference learning <br> 3) Inverse reward design | [DeepMind: Reward Modeling for Scalable Curriculum Learning](https://arxiv.org/abs/2112.13736) |\n",
       "| 5 | Calibration | Adjusting LLM outputs to better reflect uncertainty | 1) Temperature scaling <br> 2) Ensemble calibration <br> 3) Isotonic regression | [OpenAI: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.md(bedrock.sonnet_text(*task, tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea9e9377-01c1-4d0a-a42c-e5eb434b2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.get('Improve LLM performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be524a6-0f45-4129-a6e3-0f7745a051d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Popularity | Strategy                          | Description                                                                 | Examples                                      | Paper                                                                                     |\n",
       "|------------|-----------------------------------|-----------------------------------------------------------------------------|-----------------------------------------------|-------------------------------------------------------------------------------------------|\n",
       "| 1          | Fine-tuning                       | Adjusting the model parameters on a specific dataset to improve performance | GPT-3, BERT, RoBERTa                          | [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)    |\n",
       "| 2          | Data Augmentation                 | Increasing the diversity of training data without collecting new data       | Synonym Replacement, Back Translation, EDA    | [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https://arxiv.org/abs/1901.11196) |\n",
       "| 3          | Transfer Learning                 | Applying knowledge gained from one task to another related task             | ULMFiT, T5, BERT                              | [A Survey on Transfer Learning](https://ieeexplore.ieee.org/document/5288526)             |\n",
       "| 4          | Pre-training on Large Corpora     | Training the model on a large, diverse set of data before fine-tuning       | GPT-3, T5, BERT                               | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)                 |\n",
       "| 5          | Regularization and Optimization   | Techniques to prevent overfitting and improve model generalization          | Dropout, Batch Normalization, Adam Optimizer  | [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/abs/1708.02182)      |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.md(openai.text(*task, tokens=1024, stream=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1e6879e-a81b-4502-953a-034ea4a3b88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Calls | Total Time | Total Return Size | Average Time | Average Size |\n",
       "|-------|-------|------------|-------------------|--------------|--------------|\n",
       "| gtsystem.bedrock.sonnet_text | 1 | 8.53 | 1582.00 | 8.53 | 1582.00 |\n",
       "| gtsystem.openai.gpt4_text | 2 | 31.18 | 4133.00 | 15.59 | 2066.50 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.md(instrument.metrics.stats())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
