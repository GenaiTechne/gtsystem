{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a9dfb3-3586-4a7d-8df6-b336e111ecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98d3e38-543f-446c-a88a-57945bfc1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtsystem import openai, bedrock, ollama, anthropic, render, tasks, instrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba481a77-1403-46b3-9bbf-ac58036f42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks.load('../data/optimize-genai-2024-0418.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2645ec31-c96c-4704-aaa2-689aee1bcf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b89cc th {\n",
       "  font-size: 10pt;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_b89cc td {\n",
       "  text-align: left;\n",
       "  word-wrap: break-word;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       "#T_b89cc .index_name {\n",
       "  display: none;\n",
       "}\n",
       "#T_b89cc .row_heading {\n",
       "  display: none;\n",
       "}\n",
       "#T_b89cc  .blank {\n",
       "  display: none;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b89cc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b89cc_level0_col0\" class=\"col_heading level0 col0\" >Task</th>\n",
       "      <th id=\"T_b89cc_level0_col1\" class=\"col_heading level0 col1\" >System</th>\n",
       "      <th id=\"T_b89cc_level0_col2\" class=\"col_heading level0 col2\" >Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b89cc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b89cc_row0_col0\" class=\"data row0 col0\" >Reduce LLM inference cost</td>\n",
       "      <td id=\"T_b89cc_row0_col1\" class=\"data row0 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_b89cc_row0_col2\" class=\"data row0 col2\" >How to reduce LLM runtime or inference cost?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b89cc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b89cc_row1_col0\" class=\"data row1 col0\" >Improve LLM accuracy</td>\n",
       "      <td id=\"T_b89cc_row1_col1\" class=\"data row1 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_b89cc_row1_col2\" class=\"data row1 col2\" >How to improve LLM accuracy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b89cc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b89cc_row2_col0\" class=\"data row2 col0\" >Improve LLM performance</td>\n",
       "      <td id=\"T_b89cc_row2_col1\" class=\"data row2 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_b89cc_row2_col2\" class=\"data row2 col2\" >How to improve LLM performance?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b89cc_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b89cc_row3_col0\" class=\"data row3 col0\" >Effectively deploy LLMs</td>\n",
       "      <td id=\"T_b89cc_row3_col1\" class=\"data row3 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_b89cc_row3_col2\" class=\"data row3 col2\" >How to most effectively deploy LLMs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b89cc_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b89cc_row4_col0\" class=\"data row4 col0\" >Scale LLMs efficiently</td>\n",
       "      <td id=\"T_b89cc_row4_col1\" class=\"data row4 col1\" >Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.</td>\n",
       "      <td id=\"T_b89cc_row4_col2\" class=\"data row4 col2\" >How to scale LLMs efficiently?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1177fbb10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render.df(tasks.list(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821c2af3-d104-408f-8669-b6161acbd048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['240420094946-How-to-reduce-LLM-runtime-or-inference-cost.json',\n",
       " '240420101611-How-to-scale-LLMs-efficiently.json',\n",
       " '240420095000-How-to-improve-LLM-accuracy.json']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.CHAT.search('LLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc890636-f9ca-4183-bea9-6ee02dfc2050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat loaded from openai_chats/240420094946-How-to-reduce-LLM-runtime-or-inference-cost.json\n"
     ]
    }
   ],
   "source": [
    "chat = openai.CHAT.load('240420094946-How-to-reduce-LLM-runtime-or-inference-cost.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1087224b-3fa0-413a-9e0e-e2de67b0c664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "‚öôÔ∏è *Only respond with a markdown table with the top 5 most effective strategies ranked by order of most popular first. Include columns for Popularity, Strategy, Description, Examples (with 3 examples), Paper (with link to latest paper about the strategy). Be factual in your response. When facts are not available, do not generate response. Don't explain your response.*\n",
       "\n",
       "üë§ **How to reduce LLM runtime or inference cost?**\n",
       "\n",
       "üí¨\n",
       "| Popularity | Strategy                | Description                                                  | Examples                                   | Paper                                                                                     |\n",
       "|------------|-------------------------|--------------------------------------------------------------|--------------------------------------------|-------------------------------------------------------------------------------------------|\n",
       "| 1          | Quantization            | Reducing the precision of the model's parameters             | INT8, FP16, Binary                         | [Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference](https://arxiv.org/abs/1712.05877) |\n",
       "| 2          | Pruning                 | Removing redundant or non-critical parts of the model        | Weight pruning, Unit pruning, Sparse pruning | [To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression](https://arxiv.org/abs/1710.01878) |\n",
       "| 3          | Distillation            | Training a smaller model to mimic a larger one               | Teacher-student, Soft targets, FitNets     | [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)          |\n",
       "| 4          | Efficient Architectures | Designing or using models that require fewer computations    | MobileBERT, EfficientNet, TinyBERT         | [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) |\n",
       "| 5          | Caching                 | Storing previous computations to avoid redundant processing  | Layer output caching, Embedding caching, Activation caching | [Increasing the Actionability of Large-Scale Language Models Through Caching](https://arxiv.org/abs/2102.08473) |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.chat(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd99c42-56fd-45b6-b602-42fb9b0d4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.get('Improve LLM accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "212901e2-2df9-44ef-9e2d-2a7623725aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat loaded from openai_chats/240420095000-How-to-improve-LLM-accuracy.json\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Popularity | Strategy                | Description                                                  | Examples                                       | Paper                                                                                     |\n",
       "|------------|-------------------------|--------------------------------------------------------------|------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
       "| 1          | Fine-tuning             | Adjusting model parameters on specific data after pretraining| GPT-3 on legal documents, BERT on tweets, T5 on medical records | [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)  |\n",
       "| 2          | Data Augmentation       | Expanding training dataset by modifying existing data        | Synonym replacement, Back translation, Random insertion | [Data Augmentation for NLP](https://arxiv.org/abs/2105.03075)                             |\n",
       "| 3          | Transfer Learning       | Applying knowledge from one domain to a related one          | ULMFiT for sentiment analysis, BERT for question answering, ELMo for named entity recognition | [A Survey on Transfer Learning](https://ieeexplore.ieee.org/document/5288526)            |\n",
       "| 4          | Ensemble Methods        | Combining multiple models to improve performance             | Voting classifiers, Stacking, Bagging          | [Ensemble Learning to Improve Machine Learning Outcomes](https://arxiv.org/abs/2012.01284)|\n",
       "| 5          | Regularization          | Techniques to reduce overfitting                             | Dropout, L2 regularization, Early stopping     | [Regularization for Deep Learning: A Taxonomy](https://arxiv.org/abs/1710.10686)         |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.md(openai.chat(*task, tokens=1024, stream=True, reset=True, cache=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e87e88f0-6d37-427f-bc08-92c1cdd7301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = tasks.get('Improve LLM performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a10e096-9682-4d8f-9e23-f696d64b5c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Popularity | Strategy                | Description                                                  | Examples                                       | Paper                                                                                     |\n",
       "|------------|-------------------------|--------------------------------------------------------------|------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
       "| 1          | Fine-tuning             | Adjusting model parameters on specific task data             | GPT-3 on legal documents, BERT on tweets, T5 on summarization tasks | [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)  |\n",
       "| 2          | Data Augmentation       | Expanding training dataset with variations of existing data  | Synonym replacement, Back translation, Text surface transformation | [Data Augmentation for NLP](https://arxiv.org/abs/2105.03075)                             |\n",
       "| 3          | Transfer Learning       | Applying knowledge from one domain to a different but related one | Using ImageNet-trained models for medical imaging, NLP models trained on news for financial forecasting, Models trained on English for Spanish tasks | [A Survey on Transfer Learning](https://ieeexplore.ieee.org/document/5288526)            |\n",
       "| 4          | Regularization          | Techniques to prevent overfitting                            | Dropout, Early stopping, L2 regularization      | [Regularization for Deep Learning: A Taxonomy](https://arxiv.org/abs/1710.10686)          |\n",
       "| 5          | Architectural Changes   | Modifying the neural network structure                       | Introduction of attention mechanisms, Transformer models, Capsule networks | [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                            |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render.md(openai.chat(*task, tokens=1024, stream=True, reset=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
